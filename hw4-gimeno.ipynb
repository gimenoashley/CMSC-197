{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d315e5d-a79e-40dc-bec3-259aa4688e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Preprocess\"\"\"\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from email import policy\n",
    "from email.parser import BytesParser \n",
    "from collections import Counter\n",
    "\n",
    "\"\"\"Directories\"\"\"\n",
    "train = 'train'\n",
    "test = 'test'\n",
    "spam = 'train/spam'\n",
    "ham = 'train/ham'\n",
    "\n",
    "if not os.path.exists(train):\n",
    "    os.makedirs(train)\n",
    "if not os.path.exists(test):\n",
    "    os.makedirs(test)\n",
    "if not os.path.exists(spam):\n",
    "    os.makedirs(spam)\n",
    "if not os.path.exists(ham):\n",
    "    os.makedirs(ham)\n",
    "\n",
    "\"\"\"Load stop_words.txt\"\"\"\n",
    "def stop_words(filename):\n",
    "    with open(filename) as f:\n",
    "        return set(word.lower() for word in f.read().splitlines())\n",
    "        \n",
    "\"\"\"Load labels\"\"\"\n",
    "def categorization (file):\n",
    "    with open(file, 'r') as f:\n",
    "        label_map = {line.split(' ')[1].strip().replace('../', ''): line.split(' ')[0].strip() for line in f}\n",
    "    return label_map\n",
    "\n",
    "\"\"\"Clean the email body by removing HTML, alphanumeric characters, acronyms, and stopwords.\"\"\"\n",
    "def clean_email_body(body, stopwords):\n",
    "\n",
    "    #remove HTML tags\n",
    "    body = re.sub(r'<.*?>', '', body, flags=re.IGNORECASE)\n",
    "\n",
    "    #remove strings with special characters\n",
    "    body = re.sub(r\"[-()\\\"/;:<>{}`~|.!?,]\",\"\", body)\n",
    "    \n",
    "    #split email body\n",
    "    words = body.split()\n",
    "\n",
    "    #remove alphanumeric\n",
    "    cleaned_words = [\n",
    "        word.lower() for word in words\n",
    "        if word.isalpha() or \"'\" in word or \".\" in word # Keep only alphabetic words\n",
    "    ]\n",
    "\n",
    "    #remove stopwords\n",
    "    filtered_words = [\n",
    "        word for word in cleaned_words\n",
    "        if word not in stopwords\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "\"\"\"Extract the plain text body of an email from a file.\"\"\"\n",
    "def extract_email_body(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        email_message = BytesParser(policy=policy.default).parse(f)  #parses email to get the body\n",
    "    \n",
    "    body = \"\"\n",
    "    \n",
    "    #for emails with multiple parts\n",
    "    if email_message.is_multipart():\n",
    "        for part in email_message.iter_parts():\n",
    "            if part.get_content_type() == 'text/plain':  #gets text/plain type content\n",
    "                body = part.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "                break\n",
    "    else:\n",
    "        body = email_message.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "\n",
    "    return body\n",
    "\n",
    "\"\"\"Divides the dataset into train and test sets, then divide train into spam or ham\"\"\"\n",
    "def preprocess (path, labels, stopwords):         \n",
    "    folder = path\n",
    "    subfolders = os.listdir(folder)\n",
    "    \n",
    "    for subfolder in subfolders:          \n",
    "        if subfolder.isdigit():\n",
    "            subfolder_number = int(subfolder)\n",
    "            #test set\n",
    "            if subfolder_number > 70:\n",
    "                for filename in os.listdir(subfolder_path):\n",
    "                    if filename.isdigit():\n",
    "                        file_path = os.path.join(subfolder_path, filename).replace('\\\\','/')\n",
    "\n",
    "                        email_body = extract_email_body(file_path) #gets the email body\n",
    "                        #checks if email body is available\n",
    "                        if email_body:\n",
    "                            cleaned_body = clean_email_body(email_body, stopwords) #cleans email body\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                        #creates test directory\n",
    "                        test_subfolder_path = os.path.join(test, subfolder).replace('\\\\','/')\n",
    "                        if not os.path.exists(test_subfolder_path):\n",
    "                            os.makedirs(spam_subfolder_path)\n",
    "\n",
    "                        #copies the file path of the file while changing the parent directory to ensure the file stays under the same subdirectory\n",
    "                        #i.e. data/000/001 to test/000/001\n",
    "                        shutil.copy(file_path, test_subfolder_path)\n",
    "                        with open(os.path.join(test_subfolder_path, filename).replace('\\\\','/'), 'w', encoding='utf-8') as f:\n",
    "                            f.write(cleaned_body) #overwrites the content of the file with the clean version\n",
    "                        \n",
    "            else:\n",
    "                subfolder_path = os.path.join(folder, subfolder).replace('\\\\','/')\n",
    "                    \n",
    "                #create corresponding 'spam' and 'ham' subfolders within 'train'\n",
    "                for filename in os.listdir(subfolder_path):\n",
    "                    if filename.isdigit():\n",
    "                        file_path = os.path.join(subfolder_path, filename).replace('\\\\','/')\n",
    "\n",
    "                        label = labels.get(file_path, None) #gets label of the file to categorize the email\n",
    "\n",
    "                        email_body = extract_email_body(file_path)\n",
    "                        if email_body:\n",
    "                            cleaned_body = clean_email_body(email_body, stopwords)\n",
    "                        \n",
    "                        else:\n",
    "                            continue\n",
    "    \n",
    "                        if label == 'spam': #if email is spam the file is copied to the spam directory otherwise ham\n",
    "                          \n",
    "                            spam_subfolder_path = os.path.join(spam, subfolder).replace('\\\\','/')\n",
    "                            if not os.path.exists(spam_subfolder_path):\n",
    "                                os.makedirs(spam_subfolder_path)\n",
    "                            shutil.copy(file_path, spam_subfolder_path)\n",
    "                            with open(os.path.join(spam_subfolder_path, filename).replace('\\\\','/'), 'w', encoding='utf-8') as f:\n",
    "                                f.write(cleaned_body)\n",
    "                        else:\n",
    "                            ham_subfolder_path = os.path.join(ham, subfolder).replace('\\\\','/')\n",
    "                            if not os.path.exists(ham_subfolder_path):\n",
    "                                 os.makedirs(ham_subfolder_path)\n",
    "                            shutil.copy(file_path, ham_subfolder_path)\n",
    "                            with open(os.path.join(ham_subfolder_path, filename).replace('\\\\','/'), 'w', encoding='utf-8') as f:\n",
    "                                f.write(cleaned_body)\n",
    "\n",
    "\"\"\"Creates a map of words and its occurences. Outputs the first 10000 common words\"\"\"\n",
    "def count(path, size = 10000):\n",
    "    count_map = {}\n",
    "    for label in ['ham', 'spam']:\n",
    "        folder = os.path.join(path, label).replace('\\\\','/')\n",
    "        \n",
    "        for subfolder in os.listdir(folder):\n",
    "            \n",
    "            subfolder_path = os.path.join(folder, subfolder).replace('\\\\','/')\n",
    "           \n",
    "            if os.path.isdir(subfolder_path):\n",
    "                for filename in os.listdir(subfolder_path):\n",
    "                    file_path = os.path.join(subfolder_path, filename).replace('\\\\','/')\n",
    "\n",
    "                    # Check if the file exists before opening\n",
    "                    if os.path.isfile(file_path):\n",
    "                        try:\n",
    "                            with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "                                words = f.read().split()\n",
    "                                for word in words:\n",
    "                                    if word in count_map:\n",
    "                                        count_map[word] += 1  \n",
    "                                    else:\n",
    "                                        count_map[word] = 1\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "    count = Counter(count_map).most_common(size)\n",
    "    \n",
    "    return count\n",
    "\n",
    "\"\"\"Extract just the words from the count results\"\"\"\n",
    "def vocab(counter):\n",
    "    words = [word for word, _ in counter]\n",
    "    return words\n",
    "\n",
    "labels = categorization('labels')\n",
    "stopwords = stop_words('stop_words.txt')\n",
    "preprocess('data', labels, stopwords)\n",
    "counter = count('train')\n",
    "vocabulary = vocab(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea070a95-a71b-4080-873f-7692de98d48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Matrix [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Ham Matrix [[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"Create feature matrices\"\"\"\n",
    "def create_feature_matrix(path, vocabulary):\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    email_count = sum(len(files) for _, _, files in os.walk(path))  #count all email files\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)} #dictionary for words\n",
    "    \n",
    "    feature_matrix = np.zeros((email_count, vocab_size), dtype=int) #initialize empty feature matrix\n",
    "\n",
    "    email_index = 0  #initialize index for the feature matrix rows\n",
    "\n",
    "    for subfolder in os.listdir(path):\n",
    "        subfolder_path = os.path.join(path, subfolder).replace('\\\\','/')\n",
    "\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, filename).replace('\\\\','/')\n",
    "\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "                        words = f.read().split()\n",
    "                        \n",
    "                        #update the feature matrix for this email\n",
    "                        for word in words:\n",
    "                            if word in vocabulary:\n",
    "                                index = word_to_index[word]\n",
    "                                feature_matrix[email_index, index] = 1  #set to 1 if the word exists\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                    email_index += 1  #move to the next email\n",
    "\n",
    "    return feature_matrix\n",
    "\n",
    "spam_matrix=create_feature_matrix(spam, vocabulary)\n",
    "ham_matrix = create_feature_matrix(ham, vocabulary)\n",
    "print(\"Spam Matrix\", spam_matrix)\n",
    "print(\"Ham Matrix\",ham_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4182992e-bb37-43b3-86ba-fc8759e6d110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probability of Ham (P(c=ham)): 0.35\n",
      "Prior Probability of Spam (P(c=spam)): 0.65\n",
      "Number of Ham Emails: 7531\n",
      "Number of Spam Emails: 13778\n",
      "Total Number of Emails: 21309\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Computing the Priors\"\"\"\n",
    "\n",
    "def compute_priors(ham, spam):\n",
    "    #count the number of ham and spam emails\n",
    "    N_ham = sum(len(files) for _, _, files in os.walk(ham))  \n",
    "    N_spam = sum(len(files) for _, _, files in os.walk(spam))  \n",
    "    N_total = N_ham + N_spam #total no of spams\n",
    "\n",
    "    #compute prior probabilities\n",
    "    P_ham = N_ham / N_total \n",
    "    P_spam = N_spam / N_total \n",
    "\n",
    "    return P_ham, P_spam, N_ham, N_spam, N_total\n",
    "\n",
    "P_ham, P_spam, N_ham, N_spam, N_total = compute_priors(ham, spam)\n",
    "\n",
    "print(f\"Prior Probability of Ham (P(c=ham)): {P_ham:.2f}\")\n",
    "print(f\"Prior Probability of Spam (P(c=spam)): {P_spam:.2f}\")\n",
    "print(f\"Number of Ham Emails: {N_ham}\")\n",
    "print(f\"Number of Spam Emails: {N_spam}\")\n",
    "print(f\"Total Number of Emails: {N_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68dd32f0-99aa-4825-b40d-4bb501489516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham Likelihoods:\n",
      "[9.14679359e-03 6.55301027e-03 2.35035381e-04 ... 2.51823623e-05\n",
      " 1.39902013e-05 2.51823623e-05]\n",
      "Spam Likelihoods:\n",
      "[8.27196578e-03 8.80100583e-04 7.91763957e-03 ... 1.63283967e-06\n",
      " 1.46955570e-05 1.63283967e-06]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Computing the Likelihood of each word\"\"\"\n",
    "def count_word_occurrences(path, vocabulary):\n",
    "    #initialize a vector to count occurrences of each word\n",
    "    word_counts = np.zeros(len(vocabulary), dtype=int)\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "\n",
    "    for subfolder in os.listdir(path):\n",
    "        subfolder_path = os.path.join(path, subfolder).replace('\\\\','/')\n",
    "\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, filename).replace('\\\\','/')\n",
    "\n",
    "                if os.path.isfile(file_path):\n",
    "                    try:\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "                            words = f.read().split()\n",
    "                            for word in words:\n",
    "                                if word in word_to_index:  # Check if the word is in the vocabulary\n",
    "                                    index = word_to_index[word]  # Get the index of the word\n",
    "                                    word_counts[index] += 1  # Increment the count for that word\n",
    "                    except Exception as e:\n",
    "                        continue \n",
    "\n",
    "    return word_counts\n",
    "\n",
    "def compute_likelihood(word_counts, total_words, vocab_size, laplace_smoothing):\n",
    "    #apply Laplace smoothing\n",
    "    likelihoods = (word_counts + laplace_smoothing) / (total_words + (laplace_smoothing * vocab_size))\n",
    "    return likelihoods\n",
    "\n",
    "#count occurrences in ham and spam datasets\n",
    "ham_word_counts = count_word_occurrences(ham, vocabulary)\n",
    "spam_word_counts = count_word_occurrences(spam, vocabulary)\n",
    "\n",
    "#calculate total words in each dataset\n",
    "total_ham_words = ham_word_counts.sum()\n",
    "total_spam_words = spam_word_counts.sum()\n",
    "\n",
    "#compute likelihoods\n",
    "ham_likelihoods = compute_likelihood(ham_word_counts, total_ham_words, len(vocabulary), laplace_smoothing=1)\n",
    "spam_likelihoods = compute_likelihood(spam_word_counts, total_spam_words, len(vocabulary), laplace_smoothing=1)\n",
    "\n",
    "print(\"Ham Likelihoods:\")\n",
    "print(ham_likelihoods)\n",
    "print(\"Spam Likelihoods:\")\n",
    "print(spam_likelihoods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7797858-3bbb-4421-bf91-8064ac1ee896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classifying the emails\"\"\"\n",
    "def classify_email(words, ham_likelihoods, spam_likelihoods, P_ham, P_spam, vocab_size):\n",
    "    #initialize log probabilities\n",
    "    log_prob_ham = np.log(P_ham)\n",
    "    log_prob_spam = np.log(P_spam)\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "\n",
    "    #calculate the log probabilities for each word in the email\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            index = word_to_index[word]\n",
    "            log_prob_ham += np.log(ham_likelihoods[index])  #add log likelihood for ham\n",
    "            log_prob_spam += np.log(spam_likelihoods[index])  #add log likelihood for spam\n",
    "\n",
    "    return log_prob_ham, log_prob_spam\n",
    "\n",
    "def classify_emails(path, ham_likelihoods, spam_likelihoods, P_ham, P_spam):\n",
    "    results = {}\n",
    "\n",
    "    for subfolder in os.listdir(path):\n",
    "        subfolder_path = os.path.join(path, subfolder).replace('\\\\','/')\n",
    "\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, filename).replace('\\\\','/')\n",
    "\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "                        words = f.read().split()\n",
    "                        log_prob_ham, log_prob_spam = classify_email(words, ham_likelihoods, spam_likelihoods, P_ham, P_spam, len(vocabulary))\n",
    "\n",
    "                        #classify based on the higher log probability\n",
    "                        if log_prob_ham > log_prob_spam:\n",
    "                            results[file_path] = \"ham\"\n",
    "                        else:\n",
    "                            results[file_path] = \"spam\"\n",
    "\n",
    "    return results\n",
    "\n",
    "classification_results = classify_emails('test', ham_likelihoods, spam_likelihoods, P_ham, P_spam)\n",
    "\n",
    "print(classification_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c8d8b6-b563-48b3-98f9-f6323ee83612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Accuracy: 0.55\n",
      "Recall: 0.65\n",
      "Precision: 0.67\n",
      "True Positives (TP): 7190\n",
      "True Negatives (TN): 1888\n",
      "False Positives (FP): 3499\n",
      "False Negatives (FN): 3945\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Performance Evaluation\"\"\"\n",
    "def evaluate_classifier(data, labels, test, classification_results, ham_likelihoods, spam_likelihoods, P_ham, P_spam):\n",
    "    TP = 0  # True Positives\n",
    "    TN = 0  # True Negatives\n",
    "    FP = 0  # False Positives\n",
    "    FN = 0  # False Negatives\n",
    "\n",
    "    for subfolder in os.listdir(data):\n",
    "        if subfolder.isdigit():\n",
    "            subfolder_number = int(subfolder)\n",
    "            if subfolder_number > 70:\n",
    "                subfolder_path = os.path.join(data, subfolder).replace('\\\\','/')\n",
    "                for filename in os.listdir(subfolder_path):\n",
    "                    if filename.isdigit():\n",
    "                        file_path = os.path.join(subfolder_path, filename).replace('\\\\','/')\n",
    "\n",
    "                        test_path = os.path.join(test, subfolder, filename).replace('\\\\','/')\n",
    "                        \n",
    "                        actual_label = labels.get(file_path, None)\n",
    "                        predicted_label = classification_results.get(test_path, None)\n",
    "            \n",
    "                \n",
    "                        if predicted_label == \"spam\":\n",
    "                            if actual_label == \"spam\":\n",
    "                                TP += 1  #correctly classified spam\n",
    "                            else:\n",
    "                                FP += 1  #incorrectly classified ham as spam\n",
    "                        else:\n",
    "                            if actual_label == \"ham\":\n",
    "                                TN += 1  #correctly classified ham\n",
    "                            else:\n",
    "                                FN += 1  #incorrectly classified spam as ham\n",
    "\n",
    "    #calculate metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    return accuracy, recall, precision, TP, TN, FP, FN\n",
    "\n",
    "#evaluate the classifier\n",
    "accuracy, recall, precision, TP, TN, FP, FN = evaluate_classifier('data', labels, test, classification_results, ham_likelihoods, spam_likelihoods, P_ham, P_spam)\n",
    "\n",
    "#print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14480dab-1b09-4fd5-a1ea-67523663dcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
